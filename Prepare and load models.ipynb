{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6d97f8-2e5a-4b9e-98d7-90abd5e118d4",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sequence Classification and Repository Creation on Hugging Face\n",
    "\n",
    "This Jupyter notebook serves as a guide for fine-tuning the base BERT model from Hugging Face, transforming it into a model suitable for sequence classification tasks, and pushing this model into a repository on Hugging Face.\n",
    "\n",
    "The ultimate objective of this project is to train pair classifiers for the gloss of Serbian Wordnet. The classifiers will distinguish between positive/non-positive and negative/non-negative classifications. This will assist us in creating SETIWORDNET-like markings.\n",
    "\n",
    "In this part of the project, we focus on preparing the pretrained BERT model for fine-tuning. Subsequent notebooks will cover the fine-tuning process and application of the model.\n",
    "\n",
    "This notebook will take you through the following steps:\n",
    "\n",
    "1. Initial setup - Importing required libraries and setting up the environment.\n",
    "2. Loading the base BERT model from Hugging Face.\n",
    "3. Initial configuration and preparation of the model for fine-tuning.\n",
    "4. Creating a Hugging Face repository.\n",
    "5. Pushing the prepared model to the repository.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc0113-2876-41f7-a451-c0692289e2a7",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\r\n",
    "\r\n",
    "Before we begin, we need to import the necessary libraries. The `transformers` library from Hugging Face provides us with pre-trained models and tokenizers that will help us in our task. We are specifically interested in the `AutoTokenizer` and `AutoModelForSequenceClassification` classes.\r\n",
    "\r\n",
    "`AutoTokenizer` will be used to load the tokenizer corresponding to the base BERT model, while `AutoModelForSequenceClassification` is the class of the model we want to fine-tune.\r\n",
    "\r\n",
    "The `huggingface_hub` library provides tools for working with the Hugging Face model hub. We'll use the `create_repo` function to create a new repository on the Hugging Face hub where we can store our fine-tuned model.\r\n",
    "\r\n",
    "Let's import these classes and functions:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3527f3-4276-472e-9f0f-035154930492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from huggingface_hub import create_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97060c-8083-412e-82dd-a58624753ec6",
   "metadata": {},
   "source": [
    "## Preparing the Model for Fine-tuning and Creating Repositories on Hugging Face\r\n",
    "\r\n",
    "In this step, we're going to prepare two BERT models for fine-tuning: `bcms-bertic` provided by Classla, and `Bertovo-sent-base` provided by Tanor.\r\n",
    "\r\n",
    "We're training classifiers for two types of polarities - Positive and Negative. Therefore, for each polarity, we adjust the labels appropriately and load the corresponding models and tokenizers.\r\n",
    "\r\n",
    "We're also going to create a series of repositories on Hugging Face to store our models. For each iteration in our range, we create two repositories - one for `BERTicovoSENT` and one for `BERTicSENT`. These repositories will hold different versions of our fine-tuned models and their tokenizers.\r\n",
    "\r\n",
    "Each of the repositories will be private and bear the name `BERTicovoSENT{polarity}{i}` or `BERTicSENT{polarity}{i}` where `{polarity}` is either `POS` (for positive) or `NEG` (for negative), and `{i}` is the iteration number.\r\n",
    "\r\n",
    "After creating each repository, we push the models and their tokenizers to the corresponding repositories on Hugging Face. This allows us to version control our models and facilitates easy accessibility and deployment in future tasks.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c61ff4-becd-4c2f-8bc1-c15eda1cebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ba51d3d7004ae78ed73b6dc11a818d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58374bf0f184668a4d5e0732604c67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0db40af98f4a7b835eabddc0f3a08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09307683dd3d42348adb0e7b7534c8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2a570d63b240f28a2c290d3a93c9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5e5454e2e94fb884606cf3615d05ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b6084487b4353887242d460a6851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db7f149f1744c218ab1b3acd47e8817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration_range = [4,6]\n",
    "model_name_BERTic = \"classla/bcms-bertic\"\n",
    "model_name_BERTicovo = \"Tanor/Bertovo-sent-base\"\n",
    "\n",
    "for polarity in [\"NEG\",\"POS\"]:\n",
    "\n",
    "    id2label = {0: \"NON-POSITIVE\", 1: \"POSITIVE\"}\n",
    "    label2id = {\"NON-POSITIVE\": 0, \"POSITIVE\": 1}\n",
    "    if (polarity ==\"NEG\"):\n",
    "        id2label = {0: \"NON-NEGATIVE\", 1: \"NEGATIVE\"}\n",
    "        label2id = {\"NON-NEGATIVE\": 0, \"NEGATIVE\": 1}\n",
    "    tokenizer_BERTic = AutoTokenizer.from_pretrained(model_name_BERTic)\n",
    "    model_BERTic = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_BERTic, num_labels=2,  id2label=id2label, \n",
    "        label2id=label2id, )\n",
    "    tokenizer_BERTicovo = AutoTokenizer.from_pretrained(model_name_BERTicovo)\n",
    "    model_BERTicovo = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_BERTicovo, num_labels=2,  id2label=id2label, \n",
    "        label2id=label2id, )\n",
    "    for i in iteration_range:\n",
    "        model_name_out = f\"Tanor/BERTicovoSENT{polarity}{i}\"\n",
    "#        create_repo(model_name_out, private = True)\n",
    "        model_BERTicovo.push_to_hub(model_name_out)\n",
    "        tokenizer_BERTicovo.push_to_hub(model_name_out)\n",
    "        model_name_out = f\"Tanor/BERTicSENT{polarity}{i}\"\n",
    "#        create_repo(model_name_out, private = True)\n",
    "        model_BERTic.push_to_hub(model_name_out)\n",
    "        tokenizer_BERTic.push_to_hub(model_name_out)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ea9ca-2db2-4359-a98a-4362efbf0b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
