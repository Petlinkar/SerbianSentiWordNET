{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a24020-7015-4083-bdbd-1347029fa855",
   "metadata": {},
   "source": [
    "# Fine-Tuning and Performance Evaluation of BERT Models for Sentiment Classification with Early Stopping\r\n",
    "\r\n",
    "In this Jupyter notebook, we delve into the crucial steps of our Natural Language Processing (NLP) project - fine-tuning our preprocessed BERT models on the Serbian Wordnet training data, and assessing their performance. \r\n",
    "\r\n",
    "Our primary objective is to adapt BERT models to effectively classify sentiments, leveraging a semi-automated, iterative approach that uses seed words and expands them based on their relationships in WordNet. \r\n",
    "\r\n",
    "The performance evaluation metrics are instrumental in assessing the success of our fine-tuning process. We will analyze these metrics in two ways:\r\n",
    "\r\n",
    "1. **In-notebook Review:** For an immediate performance evaluation, we will print the confusion matrix and classification reports within this notebook.\r\n",
    "\r\n",
    "2. **Persistent Reports:** We'll create a lasting record of our results by storing these metrics in a separate 'reports' folder. This approach facilitates progress trackingover time, and enables comparisons among different models and fine-tuning iterations.\r\n",
    "\r\n",
    "Keep in mind that the fine-tuning and evaluation processes are iterative. Based on our results and insights, we may need to adjust our strategies and fine-tune our models \n",
    "ifferently.\r\n",
    "\r\n",
    "Throughout this notebook, we will go through:\r\n",
    "\r\n",
    "1. **Model Training:** Execution of Python scripts for fine-tuning our BERT models on the training set.\r\n",
    "2. **Model Testing:** Performance evaluation of the newly fine-tuned models on our test data.\r\n",
    "3. **Results Analysis:** Examination, interpretation, and storage of the confusion matrices and classifIn our previous work, we fine-tuned our BERT models for sentiment classification on the Serbian Wordnet training data. However, the models appeared to be overfitting. Overfitting is a common problem in machine learning where a model learns the training data too well, essentially memorizing it, rather than generalizing from it. This means that it performs poorly on unseen data, which is a big problem if we want our models to be applicable to real-world data.\r\n",
    "\r\n",
    "To overcome this issue, we're going to introduce early stopping in this notebook. Early stopping is a method used to prevent overfitting by ending the training process before the learner passes a certain point of over-specialization, i.e., before the model starts to overfit.\r\n",
    "\r\n",
    "We'll fine-tune our BERT models again, but this time, we'll include an early stopping line in our trainer call. Then, we'll evaluate the performance of these newly fine-tuned models and compare the results to the ones from the previous notebook. Our aim is to obtain models that generalize better and thus, perform better on unseen data.\r\n",
    "!\r\n",
    "Let's get started!\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7e9a2-0291-484e-bb30-1e04d1799faa",
   "metadata": {},
   "source": [
    "### Importing Required Modules\r\n",
    "\r\n",
    "In this initial code cell, we import the necessary modules that contain functions for training and testing our BERT models. The modules imported are:\r\n",
    "\r\n",
    "1. **`trainBERTovo`:** This module contains the `train_model` and `test_model` functions for handling the training and testing processes respectively. The BERT model used in this module is the \"Jerteh\" model, which is pre-trained exclusively on the Serbian language using a RoBERTa architecture. It is tailored to deal with the specificities of the Serbian language, managing everything from data preprocessing to model training, testing, and memory management for GPU use.\r\n",
    "\r\n",
    "2. **`trainBERTic`:** Similar to `trainBERTovo`, this module also contains `train_model` and `test_model` functions. However, the BERT model used in this module is the \"Classla\" model, which is based on the ELECTRA model and is multilingual, including support for Serbian among other regional languages.\r\n",
    "\r\n",
    "By encapsulating the training and testing processes within these modules, we maintain a clean and streamlined notebook. This allows us to focus on the implementation, results interpretation, and performance evaluation of the \n",
    "erstand.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670d233-eeca-4faf-90a9-a2a65d777043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import trainBERTovo\n",
    "import trainBERTic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef7d75-69bc-4e6d-a451-24947bc2cdf1",
   "metadata": {},
   "source": [
    "## Iteration 0 - Training and Testing\r\n",
    "In this section, we use the data from the 0th iteration of the semi-automatic iterative algorithm for both Positive and Negative sentiment classification to train and test our BERT models\n",
    "\n",
    "s.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed41c8-2480-47ac-9d7f-a9d66ab43d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(0, \"POS\", eval=\"f1\", epoch =32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb657f-36d8-4f0b-a0a2-2c92dfa03bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(0, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf289631-0b42-4fb0-9d1d-5f111bbcc7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(0, \"NEG\", eval=\"f1\", epoch =32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc145527-25c0-4935-8379-e17151dfe81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(0, \"NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59777033-32c4-49db-91de-71be72355654",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(0, \"POS\", eval=\"f1\", epoch = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba4a63-777d-496a-b607-1dc1b0058903",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(0, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51366b0c-fcde-48da-a07b-3dfe87c15186",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(0, \"NEG\", eval=\"f1\", epoch =32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70724-38c2-4cdc-b02d-740afc14009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(0, \"NEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308e0ba-40f7-44b4-b4a7-612be0a1f701",
   "metadata": {},
   "source": [
    "## Iteration 2 - Training and Testing\r\n",
    "In this section, we use the data from the 2nd iteration of the semi-automatic iterative algorithm for both Positive and Negative sentiment classification to train and test our BERT models.\r\n",
    "\n",
    "y.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43998619-c748-41a0-9fb0-234c6c2211ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(2, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc8971-4810-4fc3-a78b-ae026892e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(2, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d200a6-440b-4928-91e4-1b12f6487798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(2, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670ca2b-e26c-42ba-88ce-b7916e3ac6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(2, \"NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddb7b2-b082-430c-b730-c54b719ecc88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(2, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b875fbb-7a3b-4574-b8d3-a8f7fdcb736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(2, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25537dbe-ddcb-4d83-a71a-2f9ab9d80d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(2, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfd82e-8f61-416d-a742-a37723c78d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(2, \"NEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e782287-053e-41f6-b23c-c16ae91aa7ac",
   "metadata": {},
   "source": [
    "## Iteration 4 - Training and Testing\r\n",
    "In this section, we use the data from the 4th iteration of the semi-automatic iterative algorithm for both Positive and Negative sentiment classification to train and test our BERT models.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802c4dc-ceca-4e5c-9449-fe7f3c5ddbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(4, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1ba70-a5eb-473b-bf9f-805b7abdaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(4, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b24f1f-4a85-404c-8100-25c57241396e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(4, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14351b26-17a1-452e-9f26-a227badb8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(4, \"NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e3a4c-f43f-4e0b-b4ff-b6eae612b926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(4, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b15db-0af5-41cf-abe5-9f5f23886305",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(4, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d618f-0511-4b50-9093-a011eb339a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(4, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709377b2-b290-4178-9a3e-b544f5adeaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(4, \"NEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519f333-3fb7-4dfd-8674-3595bfd236ab",
   "metadata": {},
   "source": [
    "## Iteration 6 - Training and Testing\r\n",
    "In this section, we use the data from the 6th iteration of the semi-automatic iterative algorithm for both Positive and Negative sentiment classification to train and test our BERT models.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86da46f-2550-48fa-a00b-fe7afc8f5f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(6, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bebde0-10a3-4a57-8577-bb0fec616120",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(6, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebad0b-9ba7-47b7-a182-0c4d612ac890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTic.train_model(6, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd478f29-442d-468f-9c30-433947ddd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTic.test_model(6, \"NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87336ce9-510e-444d-814c-87b30742b3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(6, \"POS\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd82c49-f18c-4ec7-893e-6e221ea43fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(6, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266345c-9a23-472d-94c2-92491a3a1db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainBERTovo.train_model(6, \"NEG\", eval=\"f1\", epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa7a43-7c56-415e-a0c2-78bb9ac7d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBERTovo.test_model(6, \"NEG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
